{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime2Vec + LightGBM\n",
    "\n",
    "[paoさんのAnime2Vec](https://www.guruguru.science/competitions/21/discussions/92ec4042-bd1a-44ca-9250-d365f01a0978/)を使用して特徴量を作成してみます。  \n",
    "ただし、今回は視聴したアニメがどれかだけではなく、ユーザーが付与したレーティングも与えられています。  \n",
    "レーティングの情報を活用する方法を試し、使用しない場合と比較してみます。  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.0f} s\")\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecによる特徴量エンジニアリング\n",
    "\n",
    "以下に示す`add_w2v_features`という関数でWord2Vecを用いてアニメの特徴量とユーザーの特徴量を生成します。  \n",
    "\n",
    "1. 各ユーザーが評価したアニメのリストを作成\n",
    "2. paoさんの記事に従い、シャッフルしたリストもデータに加える\n",
    "3. Word2Vecモデルを学習する\n",
    "4. 各アニメのベクトルはそのままアニメの特徴ベクトルとなり、そのユーザーが評価したアニメのベクトルの平均をユーザーの特徴ベクトルとする\n",
    "5. 各特徴ベクトルを元のデータに結合する\n",
    "\n",
    "さて、ここで冒頭に述べたword2vecの作成にあたってスコアを考慮する方法を考えます。  \n",
    "単にスコアの値も一つのトークンとしてリストに加えてしまうのも考えられますが、今回は1～10のレーティングなので、  \n",
    "「スコアの数だけそのタイトルをリストに追加する」という手法を試してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features(train_df, val_df, test_df=None, consider_score=True):\n",
    "    anime_ids = train_df[\"anime_id\"].unique().tolist()\n",
    "    user_anime_list_dict = {\n",
    "        user_id: anime_ids.tolist() for user_id, anime_ids in train_df.groupby(\"user_id\")[\"anime_id\"]\n",
    "    }\n",
    "\n",
    "    # スコアを考慮する場合\n",
    "    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n",
    "    if consider_score:\n",
    "        title_sentence_list = []\n",
    "        for user_id, user_df in train_df.groupby(\"user_id\"):\n",
    "            user_title_sentence_list = []\n",
    "            for anime_id, anime_score in user_df[[\"anime_id\", \"score\"]].values:\n",
    "                for i in range(anime_score):\n",
    "                    user_title_sentence_list.append(anime_id)\n",
    "            title_sentence_list.append(user_title_sentence_list)\n",
    "    # スコアを考慮しない場合\n",
    "    # タイトルをそのままリストに追加する\n",
    "    else:\n",
    "        title_sentence_list = train_df.groupby(\"user_id\")[\"anime_id\"].apply(list).tolist()\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1,\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {\n",
    "        user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0)\n",
    "        for user_id, user_anime_list in user_anime_list_dict.items()\n",
    "    }\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    val_df = val_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    val_df = val_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    if test_df is not None:\n",
    "        test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "        test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習に便利な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    test_df[\"score\"] = 0  # dummy\n",
    "\n",
    "    # Initialize submission file\n",
    "    submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "    submission_df[\"score\"] = 0\n",
    "    return train_df, test_df, submission_df\n",
    "\n",
    "\n",
    "def stratified_and_group_kfold_split(train_df):\n",
    "    # https://www.guruguru.science/competitions/21/discussions/45ffc8a1-e37c-4b95-aac4-c4e338aa6a9b/\n",
    "\n",
    "    # 20%のユーザを抽出\n",
    "    n_user = train_df[\"user_id\"].nunique()\n",
    "    unseen_users = random.sample(sorted(train_df[\"user_id\"].unique()), k=n_user // 5)\n",
    "    train_df[\"unseen_user\"] = train_df[\"user_id\"].isin(unseen_users)\n",
    "    unseen_df = train_df[train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "    train_df = train_df[~train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "\n",
    "    # train_dfの80%をStratifiedKFoldで分割\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for fold_id, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n",
    "        train_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # 20%をGroupKFoldで分割\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    unseen_df[\"fold\"] = -1\n",
    "    for fold_id, (_, valid_idx) in enumerate(gkf.split(unseen_df, unseen_df[\"user_id\"], unseen_df[\"user_id\"])):\n",
    "        unseen_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # concat\n",
    "    train_df = pd.concat([train_df, unseen_df], axis=0).reset_index(drop=True)\n",
    "    train_df.drop(columns=[\"unseen_user\"], inplace=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def train(train_df, original_test_df, submission_df, consider_score=True):\n",
    "    train_df[\"oof\"] = 0\n",
    "\n",
    "    for fold in range(5):\n",
    "        # Prepare the train and validation data\n",
    "        trn_df = train_df[train_df[\"fold\"] != fold].copy()\n",
    "        val_df = train_df[train_df[\"fold\"] == fold].copy()\n",
    "\n",
    "        trn_df, val_df, test_df = add_w2v_features(\n",
    "            trn_df, val_df, original_test_df.copy(), consider_score=consider_score\n",
    "        )\n",
    "\n",
    "        # Define the features and the target\n",
    "        unused_cols = [\"user_id\", \"anime_id\", \"score\", \"fold\", \"oof\"]\n",
    "        feature_cols = [col for col in trn_df.columns if col not in unused_cols]\n",
    "        target_col = \"score\"\n",
    "\n",
    "        # Prepare the LightGBM datasets\n",
    "        lgb_train = lgb.Dataset(trn_df[feature_cols], trn_df[target_col])\n",
    "        lgb_val = lgb.Dataset(val_df[feature_cols], val_df[target_col])\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"learning_rate\": 0.1,\n",
    "            # 'reg_lambda': 1.0\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        callbacks = [lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)]\n",
    "        model_lgb = lgb.train(\n",
    "            params, lgb_train, valid_sets=[lgb_train, lgb_val], callbacks=callbacks, num_boost_round=10000\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        train_preds = model_lgb.predict(trn_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        val_preds = model_lgb.predict(val_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        test_preds = model_lgb.predict(test_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "        # Evaluate the model\n",
    "        train_score = np.sqrt(mean_squared_error(trn_df[\"score\"], train_preds))\n",
    "        val_score = np.sqrt(mean_squared_error(val_df[\"score\"], val_preds))\n",
    "        print(f\"fold{fold} RMSE: {train_score:.3f}, val RMSE: {val_score:.3f}\")\n",
    "\n",
    "        submission_df[\"score\"] += test_preds / 5\n",
    "\n",
    "        train_df.loc[train_df[\"fold\"] == fold, \"oof\"] = val_preds\n",
    "\n",
    "    total_score = np.sqrt(mean_squared_error(train_df[\"score\"], train_df[\"oof\"]))\n",
    "    print(f\"Total RMSE: {total_score}\")\n",
    "\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratingを考慮しない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load the data] done in 0 s\n",
      "[Stratified & Group split] done in 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32522\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.767403\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.2539\tvalid_1's rmse: 1.44356\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's rmse: 1.32488\tvalid_1's rmse: 1.40759\n",
      "fold0 RMSE: 1.325, val RMSE: 1.408\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32521\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.773261\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.25592\tvalid_1's rmse: 1.46087\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's rmse: 1.29915\tvalid_1's rmse: 1.4276\n",
      "fold1 RMSE: 1.299, val RMSE: 1.428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32525\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.756151\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.2583\tvalid_1's rmse: 1.30863\n",
      "[200]\ttraining's rmse: 1.19227\tvalid_1's rmse: 1.28634\n",
      "[300]\ttraining's rmse: 1.15074\tvalid_1's rmse: 1.27959\n",
      "Early stopping, best iteration is:\n",
      "[248]\ttraining's rmse: 1.17134\tvalid_1's rmse: 1.27594\n",
      "fold2 RMSE: 1.171, val RMSE: 1.276\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32527\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.766734\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.25439\tvalid_1's rmse: 1.43931\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's rmse: 1.29004\tvalid_1's rmse: 1.42403\n",
      "fold3 RMSE: 1.290, val RMSE: 1.424\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32537\n",
      "[LightGBM] [Info] Number of data points in the train set: 109122, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.780301\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.2507\tvalid_1's rmse: 1.49227\n",
      "[200]\ttraining's rmse: 1.18688\tvalid_1's rmse: 1.53936\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's rmse: 1.24726\tvalid_1's rmse: 1.47906\n",
      "fold4 RMSE: 1.247, val RMSE: 1.479\n",
      "Total RMSE: 1.4044824260406932\n",
      "[Training and evaluation with LightGBM] done in 26 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Load the data\"):\n",
    "    train_df, test_df, submission_df = load_data()\n",
    "\n",
    "with timer(\"Stratified & Group split\"):\n",
    "    train_df = stratified_and_group_kfold_split(train_df)\n",
    "\n",
    "with timer(\"Training and evaluation with LightGBM\"):\n",
    "    train(train_df, test_df, submission_df, consider_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratingを考慮する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load the data] done in 0 s\n",
      "[Stratified & Group split] done in 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32620\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.775293\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19451\tvalid_1's rmse: 1.28245\n",
      "[200]\ttraining's rmse: 1.13405\tvalid_1's rmse: 1.2614\n",
      "[300]\ttraining's rmse: 1.09303\tvalid_1's rmse: 1.25557\n",
      "[400]\ttraining's rmse: 1.05933\tvalid_1's rmse: 1.25277\n",
      "[500]\ttraining's rmse: 1.02782\tvalid_1's rmse: 1.2482\n",
      "[600]\ttraining's rmse: 0.99899\tvalid_1's rmse: 1.24644\n",
      "[700]\ttraining's rmse: 0.971144\tvalid_1's rmse: 1.2449\n",
      "[800]\ttraining's rmse: 0.945349\tvalid_1's rmse: 1.24432\n",
      "[900]\ttraining's rmse: 0.919928\tvalid_1's rmse: 1.24305\n",
      "[1000]\ttraining's rmse: 0.895846\tvalid_1's rmse: 1.24339\n",
      "Early stopping, best iteration is:\n",
      "[919]\ttraining's rmse: 0.915372\tvalid_1's rmse: 1.24281\n",
      "fold0 RMSE: 0.915, val RMSE: 1.243\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32624\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.770610\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19891\tvalid_1's rmse: 1.26763\n",
      "[200]\ttraining's rmse: 1.13697\tvalid_1's rmse: 1.24493\n",
      "[300]\ttraining's rmse: 1.09712\tvalid_1's rmse: 1.23947\n",
      "[400]\ttraining's rmse: 1.06297\tvalid_1's rmse: 1.23535\n",
      "[500]\ttraining's rmse: 1.03173\tvalid_1's rmse: 1.2327\n",
      "[600]\ttraining's rmse: 1.00284\tvalid_1's rmse: 1.23098\n",
      "[700]\ttraining's rmse: 0.976581\tvalid_1's rmse: 1.23017\n",
      "[800]\ttraining's rmse: 0.951031\tvalid_1's rmse: 1.22965\n",
      "[900]\ttraining's rmse: 0.925587\tvalid_1's rmse: 1.22921\n",
      "[1000]\ttraining's rmse: 0.90263\tvalid_1's rmse: 1.22888\n",
      "[1100]\ttraining's rmse: 0.880505\tvalid_1's rmse: 1.22916\n",
      "Early stopping, best iteration is:\n",
      "[1000]\ttraining's rmse: 0.90263\tvalid_1's rmse: 1.22888\n",
      "fold1 RMSE: 0.903, val RMSE: 1.229\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32622\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.770732\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19252\tvalid_1's rmse: 1.29142\n",
      "[200]\ttraining's rmse: 1.13343\tvalid_1's rmse: 1.26984\n",
      "[300]\ttraining's rmse: 1.09313\tvalid_1's rmse: 1.26357\n",
      "[400]\ttraining's rmse: 1.05871\tvalid_1's rmse: 1.2606\n",
      "[500]\ttraining's rmse: 1.0262\tvalid_1's rmse: 1.25791\n",
      "[600]\ttraining's rmse: 0.996273\tvalid_1's rmse: 1.25646\n",
      "[700]\ttraining's rmse: 0.968628\tvalid_1's rmse: 1.25564\n",
      "[800]\ttraining's rmse: 0.942525\tvalid_1's rmse: 1.25452\n",
      "[900]\ttraining's rmse: 0.917119\tvalid_1's rmse: 1.25386\n",
      "[1000]\ttraining's rmse: 0.894859\tvalid_1's rmse: 1.25386\n",
      "[1100]\ttraining's rmse: 0.872611\tvalid_1's rmse: 1.2531\n",
      "[1200]\ttraining's rmse: 0.85169\tvalid_1's rmse: 1.25249\n",
      "[1300]\ttraining's rmse: 0.8307\tvalid_1's rmse: 1.2521\n",
      "Early stopping, best iteration is:\n",
      "[1295]\ttraining's rmse: 0.831651\tvalid_1's rmse: 1.25206\n",
      "fold2 RMSE: 0.832, val RMSE: 1.252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32621\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.772097\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19785\tvalid_1's rmse: 1.27563\n",
      "[200]\ttraining's rmse: 1.13651\tvalid_1's rmse: 1.25257\n",
      "[300]\ttraining's rmse: 1.09654\tvalid_1's rmse: 1.24658\n",
      "[400]\ttraining's rmse: 1.06145\tvalid_1's rmse: 1.24357\n",
      "[500]\ttraining's rmse: 1.02869\tvalid_1's rmse: 1.24096\n",
      "[600]\ttraining's rmse: 0.999213\tvalid_1's rmse: 1.23912\n",
      "[700]\ttraining's rmse: 0.972005\tvalid_1's rmse: 1.23841\n",
      "[800]\ttraining's rmse: 0.946503\tvalid_1's rmse: 1.23822\n",
      "Early stopping, best iteration is:\n",
      "[735]\ttraining's rmse: 0.962595\tvalid_1's rmse: 1.23773\n",
      "fold3 RMSE: 0.963, val RMSE: 1.238\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32622\n",
      "[LightGBM] [Info] Number of data points in the train set: 109122, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score 7.755118\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.20399\tvalid_1's rmse: 1.24998\n",
      "[200]\ttraining's rmse: 1.1436\tvalid_1's rmse: 1.22863\n",
      "[300]\ttraining's rmse: 1.10472\tvalid_1's rmse: 1.22341\n",
      "[400]\ttraining's rmse: 1.06896\tvalid_1's rmse: 1.22116\n",
      "[500]\ttraining's rmse: 1.03764\tvalid_1's rmse: 1.21929\n",
      "[600]\ttraining's rmse: 1.00801\tvalid_1's rmse: 1.21673\n",
      "[700]\ttraining's rmse: 0.979385\tvalid_1's rmse: 1.21537\n",
      "[800]\ttraining's rmse: 0.95462\tvalid_1's rmse: 1.21556\n",
      "Early stopping, best iteration is:\n",
      "[711]\ttraining's rmse: 0.976391\tvalid_1's rmse: 1.21514\n",
      "fold4 RMSE: 0.976, val RMSE: 1.215\n",
      "Total RMSE: 1.2353863693751765\n",
      "[Training and evaluation with LightGBM] done in 104 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Load the data\"):\n",
    "    train_df, test_df, submission_df = load_data()\n",
    "\n",
    "with timer(\"Stratified & Group split\"):\n",
    "    train_df = stratified_and_group_kfold_split(train_df)\n",
    "\n",
    "with timer(\"Training and evaluation with LightGBM\"):\n",
    "    train(train_df, test_df, submission_df, consider_score=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratingを考慮しない場合はRMSE=1.4045  \n",
    "Ratingを考慮する場合はRMSE=1.2354(LB1.2668)  \n",
    "となりました。  \n",
    "今回もanime.csvは使用していないので、その他の特徴と組み合わせてみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
