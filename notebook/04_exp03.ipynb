{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from src.config import cfg\n",
    "from src.data import anime_id_label_encoding, load_data, user_id_label_encoding\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "pl.Config.set_tbl_rows(50)\n",
    "pl.Config.set_tbl_cols(100)\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用する最低限のデータを準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "train_df, test_df, anime_df = load_data()\n",
    "train_df, test_df = user_id_label_encoding(train_df, test_df)\n",
    "train_df, test_df, anime_df = anime_id_label_encoding(train_df, test_df, anime_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testをseen, unseenで分ける\n",
    "train_user_list = train_df[\"user_id\"].unique().to_list()\n",
    "seen_test_df = test_df.filter(pl.col(\"user_id\").is_in(train_user_list))\n",
    "unseen_test_df = test_df.filter(~pl.col(\"user_id\").is_in(train_user_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seen CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marumarukun/Documents/compe/atmacup_15/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold: 0...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.53748\n",
      "Directory already exists: ../model/lgb/seen\n",
      "RMSE for fold 0: 1.5374844241544021\n",
      "Training for fold: 1...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.56279\n",
      "Directory already exists: ../model/lgb/seen\n",
      "RMSE for fold 1: 1.5627852306976078\n",
      "Training for fold: 2...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.53774\n",
      "Directory already exists: ../model/lgb/seen\n",
      "RMSE for fold 2: 1.5377358902690097\n",
      "Training for fold: 3...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.55072\n",
      "Directory already exists: ../model/lgb/seen\n",
      "RMSE for fold 3: 1.5507228647179587\n",
      "Training for fold: 4...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.52895\n",
      "Directory already exists: ../model/lgb/seen\n",
      "RMSE for fold 4: 1.528949659351573\n",
      "Average RMSE: 1.5435356138381104\n"
     ]
    }
   ],
   "source": [
    "# seen用のCV\n",
    "skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=cfg.seed)\n",
    "\n",
    "train_df = train_df.with_columns(fold=pl.lit(-1))\n",
    "\n",
    "for fold, (_, val_index) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n",
    "    train_df[val_index, \"fold\"] = fold\n",
    "\n",
    "scores_lgb = []\n",
    "models_lgb = []\n",
    "feature_importances = []\n",
    "\n",
    "for fold in range(cfg.n_splits):\n",
    "    print(f\"Training for fold: {fold}...\")\n",
    "\n",
    "    train_data = train_df.filter(pl.col(\"fold\") != fold)\n",
    "    val_data = train_df.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    features = test_df.columns\n",
    "    target = \"score\"\n",
    "\n",
    "    lgb_train = lgb.Dataset(train_data[features].to_pandas(), train_data[target].to_pandas())\n",
    "    lgb_val = lgb.Dataset(val_data[features].to_pandas(), val_data[target].to_pandas())\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=cfg.lgb.early_stopping_rounds),\n",
    "        lgb.log_evaluation(cfg.lgb.log_evaluation_period),\n",
    "    ]\n",
    "    model_lgb = lgb.train(\n",
    "        dict(cfg.lgb.params),\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        callbacks=callbacks,\n",
    "        num_boost_round=100,\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    seen_model_dir_path = os.path.join(cfg.data.model_path, \"lgb\", \"seen\")\n",
    "    create_dir(seen_model_dir_path)\n",
    "    with open(f\"{seen_model_dir_path}/model_lgb_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_lgb, f)\n",
    "\n",
    "    # Predict the validation data\n",
    "    val_pred_lgb = model_lgb.predict(val_data[features].to_pandas(), num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score_lgb = np.sqrt(mean_squared_error(val_data[target].to_pandas(), val_pred_lgb))\n",
    "    scores_lgb.append(score_lgb)\n",
    "\n",
    "    print(f\"RMSE for fold {fold}: {score_lgb}\")\n",
    "\n",
    "    # Save feature importances\n",
    "    feature_importances.append(model_lgb.feature_importance(importance_type=\"gain\"))\n",
    "\n",
    "# Calculate the average score\n",
    "seen_average_score_lgb = np.mean(scores_lgb)\n",
    "\n",
    "print(f\"Average RMSE: {seen_average_score_lgb}\")\n",
    "\n",
    "# Calculate the average feature importance\n",
    "average_feature_importance = np.mean(feature_importances, axis=0)\n",
    "feature_importance_df = pd.DataFrame({\"feature\": features, \"importance\": average_feature_importance}).sort_values(\n",
    "    by=\"importance\", ascending=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen_test_dfに対して予測する（のちほどtest_dfにjoinする）\n",
    "seen_test_df = seen_test_df.with_columns(pl.lit(0).alias(\"score\"))\n",
    "\n",
    "for fold in range(cfg.n_splits):\n",
    "    with open(f\"{seen_model_dir_path}/model_lgb_{fold}.pkl\", \"rb\") as f:\n",
    "        model_lgb = pickle.load(f)\n",
    "    seen_test_pred_lgb = model_lgb.predict(seen_test_df[features].to_pandas(), num_iteration=model_lgb.best_iteration)\n",
    "    seen_test_df = seen_test_df.with_columns(pl.col(\"score\") + pl.Series(seen_test_pred_lgb) / cfg.n_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unseen CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold: 0...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.53013\n",
      "Directory already exists: ../model/lgb/unseen\n",
      "RMSE for fold 0: 1.530132072103847\n",
      "Training for fold: 1...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2]\tvalid_0's rmse: 1.54732\n",
      "Directory already exists: ../model/lgb/unseen\n",
      "RMSE for fold 1: 1.547320451954893\n",
      "Training for fold: 2...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 1.55414\n",
      "Directory already exists: ../model/lgb/unseen\n",
      "RMSE for fold 2: 1.5541364323221365\n",
      "Training for fold: 3...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 1.58331\n",
      "Directory already exists: ../model/lgb/unseen\n",
      "RMSE for fold 3: 1.5833075385267985\n",
      "Training for fold: 4...\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 1.57938\n",
      "Directory already exists: ../model/lgb/unseen\n",
      "RMSE for fold 4: 1.5793760046410357\n",
      "Average RMSE: 1.5588544999097422\n"
     ]
    }
   ],
   "source": [
    "# unseen用のCV\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "\n",
    "train_df = train_df.with_columns(fold=pl.lit(-1))\n",
    "\n",
    "for fold, (_, val_index) in enumerate(gkf.split(train_df, groups=train_df[\"user_id\"])):\n",
    "    train_df[val_index, \"fold\"] = fold\n",
    "\n",
    "scores_lgb = []\n",
    "models_lgb = []\n",
    "feature_importances = []\n",
    "\n",
    "for fold in range(cfg.n_splits):\n",
    "    print(f\"Training for fold: {fold}...\")\n",
    "\n",
    "    train_data = train_df.filter(pl.col(\"fold\") != fold)\n",
    "    val_data = train_df.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    features = test_df.columns\n",
    "    target = \"score\"\n",
    "\n",
    "    lgb_train = lgb.Dataset(train_data[features].to_pandas(), train_data[target].to_pandas())\n",
    "    lgb_val = lgb.Dataset(val_data[features].to_pandas(), val_data[target].to_pandas())\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=cfg.lgb.early_stopping_rounds),\n",
    "        lgb.log_evaluation(cfg.lgb.log_evaluation_period),\n",
    "    ]\n",
    "    model_lgb = lgb.train(\n",
    "        dict(cfg.lgb.params),\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        callbacks=callbacks,\n",
    "        num_boost_round=100,\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    unseen_model_dir_path = os.path.join(cfg.data.model_path, \"lgb\", \"unseen\")\n",
    "    create_dir(unseen_model_dir_path)\n",
    "    with open(f\"{unseen_model_dir_path}/model_lgb_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_lgb, f)\n",
    "\n",
    "    # Predict the validation data\n",
    "    val_pred_lgb = model_lgb.predict(val_data[features].to_pandas(), num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score_lgb = np.sqrt(mean_squared_error(val_data[target].to_pandas(), val_pred_lgb))\n",
    "    scores_lgb.append(score_lgb)\n",
    "\n",
    "    print(f\"RMSE for fold {fold}: {score_lgb}\")\n",
    "\n",
    "    # Save feature importances\n",
    "    feature_importances.append(model_lgb.feature_importance(importance_type=\"gain\"))\n",
    "\n",
    "# Calculate the average score\n",
    "unseen_average_score_lgb = np.mean(scores_lgb)\n",
    "\n",
    "print(f\"Average RMSE: {unseen_average_score_lgb}\")\n",
    "\n",
    "# Calculate the average feature importance\n",
    "average_feature_importance = np.mean(feature_importances, axis=0)\n",
    "feature_importance_df = pd.DataFrame({\"feature\": features, \"importance\": average_feature_importance}).sort_values(\n",
    "    by=\"importance\", ascending=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen_test_dfに対して予測する（のちほどtest_dfにjoinする）\n",
    "unseen_test_df = unseen_test_df.with_columns(pl.lit(0).alias(\"score\"))\n",
    "\n",
    "for fold in range(cfg.n_splits):\n",
    "    with open(f\"{unseen_model_dir_path}/model_lgb_{fold}.pkl\", \"rb\") as f:\n",
    "        model_lgb = pickle.load(f)\n",
    "    unseen_test_pred_lgb = model_lgb.predict(\n",
    "        unseen_test_df[features].to_pandas(), num_iteration=model_lgb.best_iteration\n",
    "    )\n",
    "    unseen_test_df = unseen_test_df.with_columns(pl.col(\"score\") + pl.Series(unseen_test_pred_lgb) / cfg.n_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV値を求める\n",
    "- seen CVのRSMEとunseen CVのRSMEを二乗して、重みをつけて足してルートを取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 1.5470723894337046\n"
     ]
    }
   ],
   "source": [
    "cv_score = np.sqrt((seen_average_score_lgb**2) * 0.77 + (unseen_average_score_lgb**2) * 0.23)\n",
    "\n",
    "print(f\"CV score: {cv_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submissionファイルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen_test_dfとunseen_test_dfを結合してからtest_dfにjoinしてscoresを取得する\n",
    "seen_unseen_test_df = pl.concat([seen_test_df, unseen_test_df])\n",
    "sub_score_series = test_df.join(seen_unseen_test_df, on=[\"user_id\", \"anime_id\"], how=\"left\")[\"score\"]\n",
    "\n",
    "# Predict the test data and create the submission file\n",
    "submission_df = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "submission_df = submission_df.with_columns(pl.Series(sub_score_series).alias(\"score\"))\n",
    "\n",
    "# save submission_df\n",
    "submission_df.write_csv(cfg.data.output_path + \"submission_seen_unseen_cv.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
